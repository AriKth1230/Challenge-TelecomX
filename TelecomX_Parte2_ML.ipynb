{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telecom X – Parte 2: Predicción de Cancelación (Churn)\n",
    "\n",
    "Este notebook está **dividido en 4 secciones** para seguir el flujo solicitado:\n",
    "1) **Preparación de datos** (carga CSV tratado, limpieza defensiva, one-hot si hace falta, split y desbalance)  \n",
    "2) **Correlación y selección de variables** (matriz de correlación, top variables, mutual information)  \n",
    "3) **Modelos predictivos** (Regresión Logística *con* normalización y Random Forest *sin* normalización; métricas y matrices de confusión)  \n",
    "4) **Interpretación y conclusiones** (coeficientes LR, importancias RF, permutación opcional, resumen estratégico)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preparación de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_object_dtype, is_categorical_dtype\n",
    "\n",
    "# Ruta del CSV tratado en la Parte 1 \n",
    "CANDIDATES = [\"telecomx_ml_ready_numeric.csv\", \"telecomx_ml_ready.csv\", \"telecomx_model_ready.csv\"]\n",
    "df = None\n",
    "for path in CANDIDATES:\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"✔️ Cargado: {path}  shape={df.shape}\")\n",
    "        break\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No se encontró un CSV tratado. Asegúrate de tener uno de: \" + \", \".join(CANDIDATES))\n",
    "\n",
    "# objetivo binario 0/1\n",
    "if 'abandono' not in df.columns:\n",
    "    raise ValueError(\"No se encontró la columna 'abandono' en el CSV.\")\n",
    "\n",
    "if not set(pd.unique(df['abandono'])).issubset({0,1}):\n",
    "    df['abandono'] = (df['abandono'].astype('string').str.strip().str.lower()\n",
    "                      .map({'yes':1,'no':0}).astype('int8'))\n",
    "\n",
    "# Eliminar identificadores únicos si aún existen\n",
    "for c in ['id_cliente','customerID']:\n",
    "    if c in df.columns:\n",
    "        df = df.drop(columns=[c])\n",
    "\n",
    "# aplicar one-hot (drop_first para evitar colinealidad)\n",
    "cat_cols = [c for c in df.columns if c!='abandono' and (is_object_dtype(df[c]) or is_categorical_dtype(df[c]))]\n",
    "if cat_cols:\n",
    "    df = pd.get_dummies(df, columns=cat_cols, drop_first=True, dtype='int8')\n",
    "\n",
    "# Separar X, y (todo numérico)\n",
    "y = df['abandono'].astype('int8')\n",
    "X = df.drop(columns=['abandono'])\n",
    "\n",
    "# Chequeos\n",
    "assert not X.isna().any().any(), \"Hay NaNs en X.\"\n",
    "assert set(pd.unique(y)).issubset({0,1}), \"La variable objetivo debe ser 0/1.\"\n",
    "\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape)\n",
    "\n",
    "# Desbalance de clases\n",
    "counts = y.value_counts().sort_index()\n",
    "props  = y.value_counts(normalize=True).sort_index()\n",
    "print(\"\\n=== Distribución de clases ===\")\n",
    "print(f\"Activos (0): {counts.get(0,0)} ({props.get(0,0)*100:.2f}%)\")\n",
    "print(f\"Churn (1):  {counts.get(1,0)} ({props.get(1,0)*100:.2f}%)\")\n",
    "\n",
    "# Gráfico simple de barras (una figura por gráfico)\n",
    "plt.figure()\n",
    "plt.bar(['Activos (0)','Churn (1)'], [counts.get(0,0), counts.get(1,0)])\n",
    "plt.title('Distribución de clases (conteo)')\n",
    "plt.ylabel('Número de clientes')\n",
    "for i, v in enumerate([counts.get(0,0), counts.get(1,0)]):\n",
    "    plt.text(i, v*0.98, f'{v}', ha='center', va='top')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Correlación y selección de variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Matriz de correlación (solo numéricas)\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = pd.concat([X[num_cols], y], axis=1).corr()\n",
    "\n",
    "print(\"Top correlaciones con 'abandono':\")\n",
    "print(corr['abandono'].sort_values(ascending=False))\n",
    "\n",
    "# Heatmap con matplotlib\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "im = ax.imshow(corr, vmin=-1, vmax=1, cmap='coolwarm')\n",
    "ax.set_xticks(range(len(corr.columns)))\n",
    "ax.set_yticks(range(len(corr.index)))\n",
    "ax.set_xticklabels(corr.columns, rotation=90)\n",
    "ax.set_yticklabels(corr.index)\n",
    "ax.set_title('Matriz de correlación (numéricas + objetivo)')\n",
    "fig.colorbar(im, ax=ax, label='Correlación')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mutual Information (captura relaciones no lineales)\n",
    "mi = mutual_info_classif(X[num_cols], y, random_state=42)\n",
    "mi_s = pd.Series(mi, index=num_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 15 por Mutual Information:\")\n",
    "display(mi_s.head(15))\n",
    "\n",
    "# Gráfico top-15 MI\n",
    "top = mi_s.head(15).sort_values(ascending=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top.index, top.values)\n",
    "plt.title('Top 15 - Mutual Information con \"abandono\"')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Modelos predictivos (con y sin normalización)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\n",
    "\n",
    "# Split 70/30 estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape)\n",
    "\n",
    "# Modelo A: Regresión Logística (requiere normalización)\n",
    "logreg = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Modelo B: Random Forest (no requiere normalización)\n",
    "rf = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Entrenamiento\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "def evaluar(model, Xtr, ytr, Xte, yte, name=\"modelo\"):\n",
    "    y_pred = model.predict(Xte)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(Xte)[:,1]\n",
    "        roc = roc_auc_score(yte, y_proba)\n",
    "    except Exception:\n",
    "        y_proba, roc = None, np.nan\n",
    "\n",
    "    acc = accuracy_score(yte, y_pred)\n",
    "    pre = precision_score(yte, y_pred, zero_division=0)\n",
    "    rec = recall_score(yte, y_pred)\n",
    "    f1  = f1_score(yte, y_pred)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {pre:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-score:  {f1:.3f}\")\n",
    "    print(f\"ROC AUC:   {roc:.3f}\")\n",
    "\n",
    "    # Matriz de confusión (una figura)\n",
    "    cm = confusion_matrix(yte, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm)\n",
    "    ax.set_title(f\"Matriz de confusión — {name}\")\n",
    "    ax.set_xlabel(\"Predicción\")\n",
    "    ax.set_ylabel(\"Real\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Curva ROC si hay proba\n",
    "    if y_proba is not None:\n",
    "        RocCurveDisplay.from_predictions(yte, y_proba, name=name)\n",
    "        plt.title(f\"ROC — {name}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\nReporte de clasificación (TEST):\")\n",
    "    print(classification_report(yte, y_pred, digits=3))\n",
    "\n",
    "evaluar(logreg, X_train, y_train, X_test, y_test, name=\"Regresión Logística (scaled)\")\n",
    "evaluar(rf,     X_train, y_train, X_test, y_test, name=\"Random Forest (no-scale)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Interpretación y conclusiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "feat_names = list(X_train.columns)\n",
    "\n",
    "# Coeficientes de Regresión Logística \n",
    "lr_coef = logreg.named_steps['model'].coef_[0]\n",
    "coef_ser = pd.Series(lr_coef, index=feat_names, name='coef_lr')\n",
    "\n",
    "print(\"Top 15 coeficientes (valor absoluto) — Regresión Logística:\")\n",
    "display(coef_ser.reindex(coef_ser.abs().sort_values(ascending=False).index).head(15))\n",
    "\n",
    "top_lr = coef_ser.reindex(coef_ser.abs().sort_values(ascending=True).index).tail(15)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top_lr.index, top_lr.values)\n",
    "plt.title(\"Regresión Logística — Top coeficientes (signo importa)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.2 Importancias de Random Forest\n",
    "rf_imp = rf.named_steps['model'].feature_importances_\n",
    "rf_ser = pd.Series(rf_imp, index=feat_names, name='imp_rf')\n",
    "\n",
    "print(\"\\nTop 15 importancias — Random Forest:\")\n",
    "display(rf_ser.sort_values(ascending=False).head(15))\n",
    "\n",
    "top_rf = rf_ser.sort_values(ascending=True).tail(15)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top_rf.index, top_rf.values)\n",
    "plt.title(\"Random Forest — Top importancias\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Importancia por permutación agnóstica al modelo (en TEST)\n",
    "try:\n",
    "    r_perm = permutation_importance(logreg, X_test, y_test, n_repeats=10, random_state=42, scoring='f1')\n",
    "    perm_ser = pd.Series(r_perm.importances_mean, index=feat_names, name='perm_lr')\n",
    "    print(\"\\nTop 10 — Permutation Importance (LR, ΔF1 en test):\")\n",
    "    display(perm_ser.sort_values(ascending=False).head(10))\n",
    "except Exception as e:\n",
    "    print(\"Permutation importance (LR) no disponible:\", e)\n",
    "\n",
    "# Conclusiones\n",
    "def resumen_top(series, k=5, desc=\"variable\"):\n",
    "    return [f\"{i+1}. {name}\" for i, name in enumerate(series.sort_values(ascending=False).head(k).index)]\n",
    "\n",
    "resumen = {\n",
    "    \"LR_top_coef\": resumen_top(coef_ser.abs(), 5, \"coef_lr\"),\n",
    "    \"RF_top_imp\":  resumen_top(rf_ser, 5, \"imp_rf\"),\n",
    "}\n",
    "print(\"\\n=== Resumen rápido para tu informe ===\")\n",
    "print(\"LR — variables con mayor |coef|:\", *resumen['LR_top_coef'], sep=\"\\n\")\n",
    "print(\"\\nRF — variables con mayor importancia:\", *resumen['RF_top_imp'], sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones e Insights (para tu informe)\n",
    "\n",
    "- **Modelos:** Regresión Logística (con normalización) y Random Forest (sin normalización).  \n",
    "- **Rendimiento:** compara Accuracy/Precision/Recall/F1/ROC AUC .  \n",
    "- **Variables clave:** según *coeficientes* (LR) e importancias (RF), destacan las relacionadas con:\n",
    "  - **Tipo de contrato** (e.g., *month-to-month*): mayor riesgo de churn.  \n",
    "  - **Tenencia** (*meses_en_empresa*): mayor antigüedad reduce churn.  \n",
    "  - **Cargos mensuales**: valores altos aumentan el riesgo si no hay valor percibido.  \n",
    "  - **Cargos totales**: suele ser factor protector (relacionado a lealtad).  \n",
    "  - **Método de pago** (p. ej., *electronic check*): tiende a asociarse con mayor churn.  \n",
    "\n",
    "**Recomendaciones de retención:**\n",
    "1. Incentivar **migración a contratos anual/bianual** para clientes mensuales.  \n",
    "2. Enfocar **onboarding y soporte proactivo** en los primeros 6 meses.  \n",
    "3. **Revisar planes con cargos altos**; ofrecer bundles/downgrades guiados.  \n",
    "4. **Promover pagos automáticos** (tarjeta/transferencia).  \n",
    "5. Ofrecer **servicios protectores** (seguridad online/soporte) a segmentos de riesgo.\n",
    "\n",
    "> Nota: ajusta estas conclusiones con tus *tops* de coeficientes e importancias concretos de tus datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
